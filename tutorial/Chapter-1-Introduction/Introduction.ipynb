{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    },
    "colab": {
      "name": "Introduction.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VV4Ta1rMYjHt"
      },
      "source": [
        "# Introduction to Computer Vision"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2D4QxG3YjHt"
      },
      "source": [
        "## Tasks Within Computer Vision\n",
        "* Image Classification\n",
        "* Object Detection\n",
        "* Image Segmentation\n",
        "\n",
        "* Visual Relationship Detection \n",
        "* Image Captioning\n",
        "* Image Reconstruction or Image Inpainting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_Tds7qvY86z"
      },
      "source": [
        "### Convolutional Neural Networks\n",
        "<img src=\"https://media1.s-nbcnews.com/j/MSNBC/Components/Video/__NEW/2016-02-15T13-14-33-533Z--1280x720.focal-760x428.jpg\">\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GNpVq5JqYjHt"
      },
      "source": [
        "### Convolutional Neural Networks\n",
        "<img src=\"https://github.com/mcdy143/tmls_computer_vision/blob/master/tutorial/Chapter-1-Introduction/images/cnn.jpeg?raw=1\">\n",
        "\n",
        "#### Convolutional Layer\n",
        "<img src=\"https://github.com/mcdy143/tmls_computer_vision/blob/master/tutorial/Chapter-1-Introduction/images/conv.gif?raw=1\">\n",
        "<img src=\"https://github.com/mcdy143/tmls_computer_vision/blob/master/tutorial/Chapter-1-Introduction/images/kernelmove.png?raw=1\" width=\"300\" height=\"150\">\n",
        "\n",
        "#### Pooling Layer\n",
        "<img src=\"https://github.com/mcdy143/tmls_computer_vision/blob/master/tutorial/Chapter-1-Introduction/images/pool.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6J0O01xYjHt"
      },
      "source": [
        "### Image Classification\n",
        "Image classification involves assigning a label to an entire image or photograph.\n",
        "<img src=\"https://github.com/mcdy143/tmls_computer_vision/blob/master/tutorial/Chapter-1-Introduction/images/mnist.png?raw=1\">\n",
        "<img src=\"https://github.com/mcdy143/tmls_computer_vision/blob/master/tutorial/Chapter-1-Introduction/images/cifar.png?raw=1\" width=\"600\" height=\"350\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KxL-a2WbCSE"
      },
      "source": [
        "An amazing platform for CNN : https://www.cs.ryerson.ca/~aharley/vis/conv/flat.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mZFva05vYrBJ"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAkBCAZBYjHt"
      },
      "source": [
        "### Object Detection\n",
        "Object detection is the task of image classification with localization, although an image may contain multiple objects that require localization and classification.\n",
        "<img src=\"https://github.com/mcdy143/tmls_computer_vision/blob/master/tutorial/Chapter-1-Introduction/images/obj.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqLSlC23YjHt"
      },
      "source": [
        "### Image Segmentation\n",
        "Image segmentation is the task of pixel-wise classification of objects in an image.\n",
        "<img src=\"https://github.com/mcdy143/tmls_computer_vision/blob/master/tutorial/Chapter-1-Introduction/images/segmentation.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pggG7r_MYjHt"
      },
      "source": [
        "#### General Structure\n",
        "\n",
        "1. **Encoder**: Feature extraction through a sequence of progressively narrower and deeper filters (Think: pre-trained classification network like VGG/ResNet)\n",
        "2. **Decoder**: Progressively grows the output of the encoder into a segmentation mask resembling the pixel resolution of the input image (where most customizations happen)\n",
        "3. **Skip connections**: Long range connections to draw on features at varying spatial scales to improve model accuracy\n",
        "<img src=\"https://missinglink.ai/wp-content/uploads/2019/03/SegNet-neural-network_2x.png\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyTpfUlCYjHt"
      },
      "source": [
        "#### Fully Convolutional Network (FCN) [\\[paper\\]](https://arxiv.org/abs/1411.4038)\n",
        "* Output from shallower layers have more location information.\n",
        "* Deep features can be obtained when going deeper.\n",
        "<img src=\"https://github.com/mcdy143/tmls_computer_vision/blob/master/tutorial/Chapter-1-Introduction/images/fcn.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cKR3qAUPYjHu"
      },
      "source": [
        "#### SegNet [\\[paper\\]](https://arxiv.org/abs/1511.00561)\n",
        "* uses unpooling to upsample feature maps in decoder to use and keep high frequency details intact in the segmentation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXfMD5IaYjHu"
      },
      "source": [
        "#### U-Net [\\[paper\\]](https://arxiv.org/abs/1505.04597)\n",
        "* simply concatenates the encoder feature maps to upsampled feature maps from the decoder at every stage to form a ladder like structure.\n",
        "* The architecture by its skip concatenation connections allows the decoder at each stage to learn back relevant features that are lost when pooled in the encoder.\n",
        "<img src=\"https://github.com/mcdy143/tmls_computer_vision/blob/master/tutorial/Chapter-1-Introduction/images/unet.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUa6G4MGYjHu"
      },
      "source": [
        "#### DeepLabv3+ [\\[paper\\]](https://arxiv.org/abs/1802.02611)\n",
        "* Atrous Convolution + Atrous Spatial Pyramid Pooling (ASPP): exploits multi-scale features by employing multiple parallel filters with different rates.\n",
        "<img src=\"https://github.com/mcdy143/tmls_computer_vision/blob/master/tutorial/Chapter-1-Introduction/images/atrous.png?raw=1\">\n",
        "* Improved Decoder: Encoder features are first bilinearly upsampled by a factor of 4 and then concatenated with the corresponding low-level features from the network backbone that have the same spatial resolution\n",
        "<img src=\"https://github.com/mcdy143/tmls_computer_vision/blob/master/tutorial/Chapter-1-Introduction/images/deeplab.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ0ldktRYjHu"
      },
      "source": [
        "#### PSPNet [\\[paper\\]](https://arxiv.org/abs/1612.01105)\n",
        "* Pyramid Pooling Module\n",
        "    1. CNN is used to extract feature map of the last convolutional layer\n",
        "    2. Pyramid parsing module is applied to harvest different sub-region representations, followed by upsampling and concatenation layers to form the final feature representation, which carries both local and global context information.\n",
        "    <img src=\"https://github.com/mcdy143/tmls_computer_vision/blob/master/tutorial/Chapter-1-Introduction/images/pspnet.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acCxLj1kYjHu"
      },
      "source": [
        "#### Mask-RCNN [\\[paper\\]](https://arxiv.org/abs/1703.06870)\n",
        "* Instance Segmentation\n",
        "* FCN is added on top of CNN features of Faster R-CNN to generate a binary mask (Matrix with 1s on all locations where the pixel belongs to the object and 0s elsewhere)\n",
        "<img src=\"https://github.com/mcdy143/tmls_computer_vision/blob/master/tutorial/Chapter-1-Introduction/images/maskrcnn.jpg?raw=1\">\n",
        "* R-CNN: object detection model\n",
        "    1. Generate a set of proposals for bounding boxes\n",
        "    2. Run the images in the bounding boxes through a pre-trained AlexNet and finally an SVM to see what object the image in the box is.\n",
        "    3. Run the box through bbox regression model to output tighter coordinates for the box once the object has been classified.\n",
        "    <img src=\"https://github.com/mcdy143/tmls_computer_vision/blob/master/tutorial/Chapter-1-Introduction/images/rcnn.png?raw=1\">\n",
        "* Faster R-CNN: uses Regional Proposal Network (RPN) for faster candidate bounding box generation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpFWtqXnYjHu"
      },
      "source": [
        "#### Cascade Mask R-CNN [\\[paper\\]](https://arxiv.org/abs/1906.09756)\n",
        "* Same as Mask R-CNN, segmentation branch is inserted in parallel to the detection branch of Cascade R-CNN.\n",
        "* Cascade R-CNN: object detection model. Trains multiple detection heads with multiple IoU thresholds. The output of the previous detector is fed to the next as a resampling mechanism.\n",
        "<img src=\"https://github.com/mcdy143/tmls_computer_vision/blob/master/tutorial/Chapter-1-Introduction/images/cascade.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJyHV3WaYjHu"
      },
      "source": [
        "#### HRNet [\\[paper\\]](https://arxiv.org/abs/1908.07919)\n",
        "* Existing frameworks first encode the input image as a low-resolution representation through a subnetwork that is formed by connecting high-to-low resolution convolutions in series (e.g., ResNet, VGGNet), then recover the high-resolution representation from the encoded low-resolution representation.\n",
        "<img src=\"https://github.com/mcdy143/tmls_computer_vision/blob/master/tutorial/Chapter-1-Introduction/images/hrnet1.png?raw=1\">\n",
        "* HRNet maintains high-resolution representations through parallel multi-resolution convolutions.\n",
        "<img src=\"https://github.com/mcdy143/tmls_computer_vision/blob/master/tutorial/Chapter-1-Introduction/images/hrnet2.png?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_siTzCvYjHu"
      },
      "source": [
        "#### Applications\n",
        "* Self-Driving Vehicles\n",
        "* Face Detection\n",
        "* Medical Imaging\n",
        "* Video Surveillance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzDiWJBpYjHu"
      },
      "source": [
        "## Datasets\n",
        "There are many vision datasets out there, but since our focus will be semantic segmentation, we'll introduce the following datasets.\n",
        "\n",
        "### Cityscapes\n",
        "30 classes in urban street scenes.\n",
        "<img src=\"https://github.com/mcdy143/tmls_computer_vision/blob/master/tutorial/Chapter-1-Introduction/images/cityscape.png?raw=1\">\n",
        "### PASCAL\n",
        "20 classes. The train/val data has 11,530 images containing 27,450 ROI annotated objects and 6,929 segmentations.\n",
        "<img src=\"https://github.com/mcdy143/tmls_computer_vision/blob/master/tutorial/Chapter-1-Introduction/images/pascal.png?raw=1\">\n",
        "### COCO\n",
        "91 object types. More than 300k photos and 40 scene categories.\n",
        "<img src=\"https://github.com/mcdy143/tmls_computer_vision/blob/master/tutorial/Chapter-1-Introduction/images/cocodata.png?raw=1\">\n",
        "### ADE20K\n",
        "3,169 object classes across 1,072 complex everyday scenes. 25k images with an average of 19.5 annotated instances and 10.5 annotated object classes per image.\n",
        "<img src=\"https://github.com/mcdy143/tmls_computer_vision/blob/master/tutorial/Chapter-1-Introduction/images/ade20k.png?raw=1\">\n",
        "This is the dataset we'll be focusing our efforts on. \n",
        "\n",
        "[Here](https://github.com/mosdragon/kdd2020/tree/master/datasets) are the instructions for downloading the dataset.\n",
        "\n"
      ]
    }
  ]
}